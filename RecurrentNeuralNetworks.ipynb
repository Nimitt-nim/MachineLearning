{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "\n",
    "In these neural networks the output is influenced by previous inputs too, so useful for sequential data. For example, predicting next word in the sentence. Basically, they have loops which feeds back to the same layer.\n",
    "\n",
    "$ h_{t} = W_{hh}x_{t-1} + W_{hx}x_{t}$ \\\n",
    "$ y_{t} = W_{yh}h_{t}$\n",
    "\n",
    "But these Neural Networks are very hard to train because of vanishing and exploding gradient problem. So, we have its updated version LSTM : Long Short Term Memory Networks where we don't consider all the previous inputs but channelize them into short and long channels using Forget Gate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM cell\n",
    "\n",
    "class LSTMcell:\n",
    "\n",
    "    def __init__(self,m,r):\n",
    "\n",
    "        # percentage long term to remember\n",
    "        self.w_input_1 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_1 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_1 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        self.w_input_2 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_2 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_2 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        self.w_input_3 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_3 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_3 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # tanh\n",
    "\n",
    "        # New short\n",
    "        self.w_input_4 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_4 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_4 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigmoid\n",
    "        self.parameters = torch.tensor([self.w_input_1,self.w_input_2,self.w_input_3,self.w_input_4,\n",
    "                           self.w_short_1,self.w_short_2,self.w_short_3,self.w_short_4,\n",
    "                           self.b_1,self.b_2,self.b_3,self.b_4],requires_grad=True)\n",
    "\n",
    "\n",
    "    def integrate(self, input, short, long):\n",
    "        # percentage long term to remember\n",
    "        \n",
    "        o1 = torch.matmul(self.w_input_1, input)\n",
    "        o2 = torch.matmul(self.w_short_1, short)\n",
    "        o3 = torch.mul(long,torch.sigmoid(o1+o2 + self.b_1))\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        o4 = torch.matmul(self.w_input_2, input)\n",
    "        o5 = torch.matmul(self.w_short_2, short) \n",
    "        o6 = torch.sigmoid(o4+o5 + self.b_2)\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        o7 = torch.matmul(self.w_input_3, input)\n",
    "        o8 = torch.matmul(self.w_short_3, short)\n",
    "        o9 = torch.tanh(o7+o8+ self.b_3) \n",
    "\n",
    "        newLong = o3 + torch.mul(o6,o9)\n",
    "        \n",
    "\n",
    "        # new short\n",
    "        o10 = torch.matmul(self.w_input_4, input)\n",
    "        o11 = torch.matmul(self.w_short_4, short)\n",
    "        o12 = torch.sigmoid(o11+o10 + self.b_4)\n",
    "        \n",
    "        newShort = torch.mul(o12,(torch.tanh(newLong)))\n",
    "\n",
    "        return newShort,newLong\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.ms = []\n",
    "        self.rs = []\n",
    "        self.parameters = []\n",
    "        self.loss_function = nn.MSELoss()\n",
    "    \n",
    "    def add_layer(self,m,r,n): \n",
    "        # m : Input dimension \n",
    "        # n :number of cells in the layer \n",
    "        # r : output dimension\n",
    "        self.layers.append([])  \n",
    "        self.ms.append(m)\n",
    "        self.rs.append(r)   \n",
    "        for i in range(n):\n",
    "            newcell = LSTMcell(m,r)\n",
    "            self.layers[-1].append(newcell)\n",
    "            self.parameters.append(newcell.parameters)\n",
    "\n",
    "    def forward(self, input):\n",
    "        l = len(self.layers)\n",
    "        short = torch.zeros((l, self.rs[0],1), dtype = torch.float32)\n",
    "        long = torch.zeros((l,self.rs[0],1),dtype = torch.float32)\n",
    "        for i in range(len(input)):\n",
    "            current_input = input[i].unsqueeze(1)\n",
    "            for j in range(l):\n",
    "                short_,long_ = short[j],long[j]\n",
    "                for lstm_cell in self.layers[j]:\n",
    "                    short_,long_ = lstm_cell.integrate(current_input,short_,long_)\n",
    "                short[j],long[j] = short_,long_\n",
    "                current_input = short_\n",
    "        return short_\n",
    "    \n",
    "    def out(self,input):\n",
    "        out = []\n",
    "        for i in range(len(input)):\n",
    "            out.append(self.forward(input[i]))\n",
    "        return torch.tensor(out,requires_grad=True).unsqueeze(-1)\n",
    "    \n",
    "    def train(self,X,y,max_epochs):\n",
    "        optimizer = optim.Adam(self.parameters)\n",
    "        for i in range(max_epochs):\n",
    "            out = self.out(X)\n",
    "            loss = self.loss_function(out,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0,0],[1,1]],dtype = torch.float32).unsqueeze(-1)\n",
    "y = torch.tensor([0,1],dtype = torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = LSTM()\n",
    "lst.add_layer(1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2411],\n",
       "        [0.3355]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst.train(X,y,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2411],\n",
       "        [0.3355]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_hat = lst.out(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7510],\n",
       "        [-0.3948]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat - y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
