{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "\n",
    "In these neural networks the output is influenced by previous inputs too, so useful for sequential data. For example, predicting next word in the sentence. Basically, they have loops which feeds back to the same layer.\n",
    "\n",
    "$ h_{t} = W_{hh}x_{t-1} + W_{hx}x_{t}$ \\\n",
    "$ y_{t} = W_{yh}h_{t}$\n",
    "\n",
    "But these Neural Networks are very hard to train because of vanishing and exploding gradient problem. So, we have its updated version LSTM : Long Short Term Memory Networks where we don't consider all the previous inputs but channelize them into short and long channels using Forget Gate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMcell:\n",
    "\n",
    "    def __init__(self,m,r):\n",
    "\n",
    "        # percentage long term to remember\n",
    "        self.w_input_1 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.w_short_1 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.b_1 = nn.Parameter(torch.rand(r),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        self.w_input_2 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.w_short_2 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.b_2 = nn.Parameter(torch.rand(r),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        self.w_input_3 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.w_short_3 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.b_3 = nn.Parameter(torch.rand(r),requires_grad=True)\n",
    "        # tanh\n",
    "\n",
    "        # New short\n",
    "        self.w_input_4 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.w_short_4 = nn.Parameter(torch.rand(m,r),requires_grad=True)\n",
    "        self.b_4 = nn.Parameter(torch.rand(r),requires_grad=True)\n",
    "        # sigmoid\n",
    "\n",
    "    def integrate(self, input, short, long):\n",
    "        # percentage long term to remember\n",
    "        o1 = torch.matmul(self.w_input_1, input)\n",
    "        o2 = torch.matmul(self.w_input_1, short)\n",
    "        o3 = torch.matmul(long,torch.sigmoid(o1+o2) + self.b_1)\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        o4 = torch.matmul(self.w_input_2, input)\n",
    "        o5 = torch.matmul(self.w_input_2, short) \n",
    "        o6 = torch.matmul(long,torch.sigmoid(o4+o5) + self.b_2)\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        o7 = torch.matmul(self.w_input_3, input)\n",
    "        o8 = torch.matmul(self.w_input_3, short)\n",
    "        o9 = torch.matmul(long,torch.tanh(o7+o8) + self.b_3)\n",
    "\n",
    "        newLong = o3 + torch.matmul(o6,o9)\n",
    "\n",
    "        # new short\n",
    "        o10 = torch.matmul(self.w_input_1, input)\n",
    "        o11 = torch.matmul(self.w_input_1, short)\n",
    "        o12 = torch.matmul(long,torch.sigmoid(o11+o10) + self.b_4)\n",
    "        \n",
    "        newShort = o12*(torch.tanh(newLong))\n",
    "\n",
    "        return [newShort,newLong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self,m,r,n):\n",
    "        self.layers.append([])\n",
    "        for i in range(n):\n",
    "            self.layers[-1].append(LSTMcell(m,r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
