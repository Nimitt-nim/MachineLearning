{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from einops import rearrange\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image('Datasets/dog.jpg')\n",
    "scaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\n",
    "img_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\n",
    "img_scaled = torch.tensor(img_scaled)\n",
    "cropped_img = torchvision.transforms.functional.crop(img_scaled,600,800,300,300)\n",
    "cropped_img = rearrange(cropped_img,'c h w -> h w c')\n",
    "cropped_img = torch.tensor(cropped_img,dtype = torch.float32)\n",
    "original_img = cropped_img.clone()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_image_patch(img,x,y,z,patch_size):\n",
    "    img_copy = img.clone()\n",
    "    for i in range(patch_size):\n",
    "        for j in range(patch_size):\n",
    "                for k in range(z):\n",
    "                    img_copy[x+i][y+j][k] = torch.nan\n",
    "    return img_copy\n",
    "\n",
    "def factorize_matrix(A, r):\n",
    "    mask = ~torch.isnan(A)\n",
    "    m,n = A.shape\n",
    "    W = torch.rand(m,r,requires_grad=True)\n",
    "    H = torch.rand(r,n,requires_grad=True)\n",
    "    optimizer = optim.Adam([W,H],lr = 0.01)\n",
    "\n",
    "    max_epochs = 1000\n",
    "    for i in range(max_epochs):\n",
    "        loss = torch.norm((A - torch.mm(W,H))[mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return W,H, loss\n",
    "\n",
    "def image_reconstrunction_matrix_factorization(original_img,masked_img):\n",
    "    W,H,loss = factorize_matrix(rearrange(masked_img, 'h w c -> h (w c)'), 50)\n",
    "    reconstructed_img = torch.mm(W,H).detach()\n",
    "    reconstructed_img = reconstructed_img.reshape(masked_img.shape[0],masked_img.shape[1],masked_img.shape[2])\n",
    "\n",
    "    # scaler_img = preprocessing.MinMaxScaler().fit(reconstructed_img.reshape(-1, 1))\n",
    "    # reconstructed_img = scaler_img.transform(reconstructed_img.reshape(-1, 1)).reshape(masked_img.shape)\n",
    "    # reconstructed_img = torch.tensor(reconstructed_img)\n",
    "    \n",
    "    fig,axs = plt.subplots(1,2)\n",
    "    axs[0][0].imshow(masked_img)\n",
    "    axs[0][1].imshow(reconstructed_img)\n",
    "    # axs[1][0].imshow(cropped_img[x_y_s[i][0]:x_y_s[i][0]+patch_sizes[i],x_y_s[i][1]:x_y_s[i][1]+patch_sizes[i],:])\n",
    "    # axs[1][1].imshow(reconstructed_img[x_y_s[i][0]:x_y_s[i][0]+patch_sizes[i],x_y_s[i][1]:x_y_s[i][1]+patch_sizes[i],:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression + RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed_and_original_image(original_img, masked_img, net, X, title=\"\"):\n",
    "    \"\"\"\n",
    "    net: torch.nn.Module\n",
    "    X: torch.Tensor of shape (num_samples, 2)\n",
    "    Y: torch.Tensor of shape (num_samples, 3)\n",
    "    \"\"\"\n",
    "    height, width, num_channels = original_img.shape\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net(X).reshape(height,width,num_channels)\n",
    "        #outputs = outputs.permute(1, 2, 0)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])\n",
    "\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    ax2 = plt.subplot(gs[2])\n",
    "\n",
    "    ax0.imshow(outputs)\n",
    "    ax0.set_title(\"Reconstructed Image\")\n",
    "    \n",
    "\n",
    "    ax1.imshow(original_img.cpu())\n",
    "    ax1.set_title(\"Original Image\")\n",
    "\n",
    "    ax2.imshow(masked_img.cpu())\n",
    "    ax2.set_title(\"Masked Image\")\n",
    "\n",
    "    \n",
    "    for a in [ax0, ax1, ax2]:\n",
    "        a.axis(\"off\")\n",
    "\n",
    "def create_rff_features(X, num_features, sigma):\n",
    "    from sklearn.kernel_approximation import RBFSampler\n",
    "    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n",
    "    X = X.cpu().numpy()\n",
    "    X = rff.fit_transform(X)\n",
    "    return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "def train(net, lr, X, Y, epochs, verbose=True):\n",
    "    \"\"\"\n",
    "    net: torch.nn.Module\n",
    "    lr: float\n",
    "    X: torch.Tensor of shape (known_pixels, 2) // (x,y)\n",
    "    Y: torch.Tensor of shape (known_pixels, 3) // (r,g,b)\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    if verbose :\n",
    "            print(f\"Initial loss: {loss.item():.6f}\")\n",
    "            verbose = 2\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if verbose :\n",
    "        print(f\"Final loss: {loss.item():.6f}\")\n",
    "    return loss.item()\n",
    "\n",
    "def mask_image_patch(img,x,y,z,patch_size):\n",
    "    img_copy = img.clone()\n",
    "    for i in range(patch_size):\n",
    "        for j in range(patch_size):\n",
    "                for k in range(z):\n",
    "                    img_copy[x+i][y+j][k] = torch.nan\n",
    "    return img_copy\n",
    "\n",
    "def create_mask(t,x,y,patch_size):\n",
    "    mask = torch.full(t.shape,True)\n",
    "    z = t.shape[2]\n",
    "    for i in range(patch_size):\n",
    "        for j in range(patch_size):\n",
    "                for k in range(z):\n",
    "                    mask[x+i][y+j][k] = False\n",
    "    return mask\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "def create_coordinate_map(img):\n",
    "    \"\"\"\n",
    "    img: torch.Tensor of shape (num_channels, height, width)\n",
    "    \n",
    "    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    height, width, num_channels = img.shape\n",
    "    X = torch.empty((img.shape[0],img.shape[1],2))\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            X[i][j][0] = i\n",
    "            X[i][j][1] = j\n",
    "    return X.reshape(-1,2)\n",
    "\n",
    "def stack_itself(t, n):\n",
    "    # Expand the tensor along a new dimension\n",
    "    stacked_t = t.unsqueeze(1).expand(-1, n, -1)\n",
    "    return stacked_t.squeeze()\n",
    "\n",
    "def scale(img):\n",
    "    # MinMaxScaler from -1 to 1\n",
    "    scaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(img)\n",
    "    img_scaled = scaler_X.transform(img)\n",
    "    img_scaled = torch.tensor(img_scaled)\n",
    "    img_scaled = img_scaled.float()\n",
    "    return img_scaled\n",
    "\n",
    "def image_reconstrunction_linear_rff(original_img,masked_img):\n",
    "    \n",
    "    y = original_img.clone().reshape(-1,3)\n",
    "    X = create_coordinate_map(original_img)\n",
    "    mask = ~torch.isnan(masked_img).reshape(-1,3)\n",
    "    X_train = X[mask[:,0:2]].reshape(-1,2)\n",
    "    y_train = y[mask].reshape(-1,3)\n",
    "\n",
    "    # Without RFF\n",
    "    # net = LinearModel(2,3)\n",
    "    # train(net,0.01,X_train,y_train,1000)\n",
    "    # plot_reconstructed_and_original_image(original_img, net, X, title=\"Reconstructed Image\")\n",
    "\n",
    "    # With RFF\n",
    "    X_rff = create_rff_features(X, 100, 0.008)\n",
    "    mask_rff = stack_itself(mask[:,0].unsqueeze(1),100)\n",
    "    X_rff_train = X_rff[mask_rff].reshape(-1,100)\n",
    "    y_rff_train = y[mask].reshape(-1,3)\n",
    "\n",
    "    netrff = LinearModel(X_rff_train.shape[1], 3)\n",
    "    train(netrff, 0.005, X_rff_train, y_rff_train, 1000)\n",
    "    plot_reconstructed_and_original_image(original_img, masked_img, netrff, X_rff, title=\"Reconstructed Image with RFF Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 30\n",
    "x_y_s = [[10,10,3],[90,150,3],[140,60,3]]\n",
    "\n",
    "for i in range(len(x_y_s)):\n",
    "    masked_img = mask_image_patch(original_img,x_y_s[0],x_y_s[1],3,patch_size)\n",
    "    image_reconstrunction_matrix_factorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = 50\n",
    "y_start = 50\n",
    "# n1 = [30]\n",
    "patch_sizes = [20,40,60,80,100]\n",
    "for i in range(len(patch_sizes)):\n",
    "    masked_img = mask_image_patch(original_img,x_start,y_start,3,patch_sizes[i])\n",
    "    image_reconstrunction_matrix_factorization(original_img,masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = 50\n",
    "y_start = 50\n",
    "# n1 = [30]\n",
    "patch_sizes = [20,40,60,80,100]\n",
    "for i in range(len(patch_sizes)):\n",
    "    masked_img = mask_image_patch(original_img,x_start,y_start,3,patch_sizes[i])\n",
    "    image_reconstrunction_linear_rff(original_img,masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 30\n",
    "x_y_s = [[10,10],[90,150],[140,60]]\n",
    "for i in range(len(x_y_s)):\n",
    "    masked_img = mask_image_patch(original_img,x_y_s[0],x_y_s[1],3,patch_size)\n",
    "    image_reconstrunction_linear_rff(original_img,masked_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
